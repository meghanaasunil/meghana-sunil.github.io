---
layout: homepage
---


<div class="row">
    <div class="col-md-7">
    <div class="block"></div>
      <p>Hi, I'm Meghana!</p>
      <p>I'm a B.Tech student in Computer Science and Engineering (Data Science) at VIT University, where I am advised by Dr. Joe Dhanith. My research focuses on large multimodal models for reasoning, spanning representation learning, domain generalization, and cross-modal intelligence. I'm broadly interested in how multimodal systems learn structure across vision, language, and neural signals, and how these models become more robust, interpretable, and adaptable.</p>
      <p>Currently, I'm a Machine Learning Intern at Nagasaki University advised by Dr. Muthu Subash Kavitha, where I work on improving implicit reasoning in large multimodal models. In parallel, I also work with Dr. Min Xu at the Xu Lab at Carnegie Mellon University on multimodal learning and medical AI, focusing on EEG–fNIRS fusion through state-space–based sequence modeling. Prior to this, I was a Machine Learning Intern at MulticoreWare Inc., where I worked on model optimization and post-training quantization, and on finetuning large-scale transformers for efficient deployment. I also interned at the Center for Neuroinformatics with Dr. Jeetashree Aparajita, where I worked on multimodal EEG–fNIRS learning fusion pipelines for cognitive stress analysis. I am a recipient of the Raman Research Award at VIT for my research contributions.</p>
      <p>Outside of academics, I love dancing. I lead major cultural events on campus and serve as President of VITc's dance club, where I choreograph, perform, and build spaces for artistic expression.</p>

      <p><a href="mailto:meghanaa.sunil@gmail.com"><u>email</u></a> / <a href="https://www.linkedin.com/in/meghana-sunil"><u>linkedin</u></a> / <a href="https://github.com/meghanaasunil"><u>github</u></a></p>
    </div>
    <div class="col-md-5">
        <img src="/assets/img/me/Meghana_sunil.jpeg?v=2" alt="Meghana Sunil" >
    </div>
</div>
<div>&nbsp;</div>

<h2>Selected Works</h2>

<div class="row" style="margin-bottom: 30px;">
    <div class="col-md-4">
        <img src="/assets/img/research/eeg-fnirs.jpg" alt="EEG-fNIRS Architecture" style="max-width: 100%; height: auto;">
    </div>
    <div class="col-md-8">
        <h4><strong>Graph-Guided Cross-Modal Representation Learning of EEG–fNIRS via Spatio-Temporal Transformers for Stress Analysis</strong></h4>
        <p><strong>Meghana Sunil</strong></p>
        <p><i>Submitted: Computer Methods and Programs in Biomedicine</i> | Advisor: Dr. Jeetashree Aparajeeta</p>
        <p>Proposed CM-GGT, a cross-modal gated fusion framework integrating a spectrogram-transformer EEG encoder and graph neural network fNIRS encoder. Achieved 93.2% accuracy and 0.967 AUC on simultaneous EEG–fNIRS recordings, outperforming 11 state-of-the-art baselines.</p>
        <p>[<a href="#">Paper</a>] [<a href="#">Code</a>]</p>
    </div>
</div>

<div class="row" style="margin-bottom: 30px;">
    <div class="col-md-4">
        <img src="/assets/img/research/lung-cancer.jpg" alt="Lung Cancer Diagnosis Architecture" style="max-width: 100%; height: auto;">
    </div>
    <div class="col-md-8">
        <h4><strong>Encouraging Discriminative Attention Through Contrastive Explainability Learning for Lung Cancer Diagnosis</strong></h4>
        <p><strong>Meghana Sunil</strong></p>
        <p><i>IEEE Access</i> | Advisor: Dr. Natarajan B</p>
        <p>Developed a novel Contrastive Explainability Learning (CEL) framework for lung cancer classification, achieving >98% accuracy and F1-score on the IQ-OTH/NCCD dataset. Enforcing self-supervised Grad-CAM contrastive constraints in CNNs to enhance feature attribution and discriminative attention in medical imaging.</p>
        <p>[<a href="#">Paper</a>] [<a href="#">Code</a>]</p>
    </div>
</div>

<p>For more research work, please visit my <a href="/research">Research</a> page.</p>

